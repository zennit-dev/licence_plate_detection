# Optimizers configuration
# ========================


#------------------------------------------------------------------------------
# Adam Optimizer
#------------------------------------------------------------------------------
adam:
  learning_rate: 0.001
  beta_1: 0.9
  beta_2: 0.999
  epsilon: 1e-07
  amsgrad: false


#------------------------------------------------------------------------------
# SGD Optimizer
#------------------------------------------------------------------------------
sgd:
  learning_rate: 0.01
  momentum: 0.0
  nesterov: false

#------------------------------------------------------------------------------
# RMSprop Optimizer
#------------------------------------------------------------------------------
rmsprop:
  learning_rate: 0.001
  rho: 0.9
  momentum: 0.0
  epsilon: 1e-07
  centered: false
